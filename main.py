import asyncio
import os
import re
from urllib.parse import urlparse, urljoin
from pathlib import Path

# --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å ---
from rich.console import Console
from rich.prompt import Prompt, IntPrompt, Confirm
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeRemainingColumn
# ------------------------------------

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from pypdf import PdfWriter

# --- –ù–ê–°–¢–†–û–ô–ö–ò –ú–ê–°–ö–ò–†–û–í–ö–ò ---
FAKE_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
EXTRA_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Sec-Ch-Ua': '"Chromium";v="118", "Google Chrome";v="118", "Not=A?Brand";v="99"',
    'Sec-Ch-Ua-Mobile': '?0',
    'Sec-Ch-Ua-Platform': '"Windows"',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-User': '?1',
    'Upgrade-Insecure-Requests': '1',
}
# -----------------------------

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Å–æ–ª—å Rich –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
console = Console()

def get_user_config() -> dict:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ."""
    console.print(Panel.fit("[bold cyan]–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ Web-To-PDF Crawler![/bold cyan]\n–î–∞–≤–∞–π—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—Ö–æ–¥–∞ —Å–∞–π—Ç–∞.", title="–ù–∞—Å—Ç—Ä–æ–π–∫–∞"))

    start_url = Prompt.ask("[yellow]–í–≤–µ–¥–∏—Ç–µ —Å—Ç–∞—Ä—Ç–æ–≤—ã–π URL[/yellow]", default="https://kkrugley.github.io/")

    # –£–¥–æ–±–Ω—ã–π –≤—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ —Å–∫—Ä–∞–ø–ø–∏–Ω–≥–∞ –ø–æ –Ω–æ–º–µ—Ä—É
    crawl_mode_map = {"1": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π", "2": "Sitemap.xml"}
    crawl_mode_choice = Prompt.ask(
        "[yellow]–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Å–∫—Ä–∞–ø–ø–∏–Ω–≥–∞[/yellow] ([bold]1[/bold]=–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π, [bold]2[/bold]=Sitemap.xml)",
        choices=["1", "2"],
        default="1"
    )
    crawl_mode = crawl_mode_map[crawl_mode_choice]

    max_depth = IntPrompt.ask("[yellow]–í–≤–µ–¥–∏—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É –æ–±—Ö–æ–¥–∞[/yellow]", default=1)
    request_delay = IntPrompt.ask("[yellow]–í–≤–µ–¥–∏—Ç–µ –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫)[/yellow]", default=2)

    export_format = Prompt.ask(
        "[yellow]–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF –∏–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç?[/yellow]",
        choices=["PDF", "Text"],
        default="Text"
    )

    merge_files = Confirm.ask("[yellow]–û–±—ä–µ–¥–∏–Ω—è—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã?[/yellow]", default=True)

    output_dir = "output_pdfs" if export_format == "PDF" else "output_texts"
    merged_filename = "merged_output.pdf" if export_format == "PDF" else "merged_output.md"

    return {
        "START_URL": start_url,
        "CRAWL_MODE": crawl_mode,
        "MAX_DEPTH": max_depth,
        "REQUEST_DELAY": request_delay,
        "EXPORT_FORMAT": export_format,
        "OUTPUT_DIR": output_dir,
        "MERGED_FILENAME": merged_filename,
        "DELETE_INDIVIDUAL_FILES": True,
        "MERGE_FILES": merge_files,
    }

def sanitize_filename(title: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É, —á—Ç–æ–±—ã –æ–Ω–∞ –±—ã–ª–∞ –≤–∞–ª–∏–¥–Ω—ã–º –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞."""
    if not title:
        title = "no_title"
    # –£–¥–∞–ª—è–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è Windows/Linux/macOS
    sanitized = re.sub(r'[\\/*?:"<>|]', "", title)
    # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ—Ñ–∏—Å–æ–≤
    sanitized = re.sub(r'\s+', '_', sanitized)
    sanitized = re.sub(r'__+', '_', sanitized)
    return sanitized[:100] # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞

def is_valid_url(url: str, start_hostname: str, start_path: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ URL —Å—Ç—Ä–æ–≥–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
    1. –¢–æ—Ç –∂–µ —Ö–æ—Å—Ç (–±–µ–∑ –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤).
    2. –ü—É—Ç—å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç–æ–≥–æ –∂–µ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏.
    3. –ù–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Å—ã–ª–∫–æ–π –Ω–∞ —Ñ–∞–π–ª.
    """
    try:
        parsed_url = urlparse(url)
        if parsed_url.scheme not in ['http', 'https']: return False
        if parsed_url.netloc != start_hostname: return False
        if not parsed_url.path.startswith(start_path): return False
        if any(parsed_url.path.lower().endswith(ext) for ext in ['.pdf', '.zip', '.jpg', '.png', '.gif', '.xml', '.rss', '.gz']): return False
        return True
    except ValueError:
        return False


import aiohttp
import xml.etree.ElementTree as ET

async def fetch_sitemap_links(sitemap_url):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç sitemap.xml, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫."""
    links = []
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(sitemap_url, headers=EXTRA_HEADERS) as resp:
                if resp.status == 200:
                    text = await resp.text()
                    try:
                        root = ET.fromstring(text)
                        for url in root.findall('.//{*}loc'):
                            if url.text:
                                loc = url.text.strip()
                                if loc:
                                    links.append(loc)
                    except Exception:
                        pass
    except Exception:
        pass
    return links

async def main(CONFIG: dict):
    """–ì–ª–∞–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π."""
    output_path = Path(CONFIG["OUTPUT_DIR"])
    output_path.mkdir(exist_ok=True)

    start_url = CONFIG["START_URL"]
    parsed_start_url = urlparse(start_url)
    start_hostname = parsed_start_url.netloc

    start_path_obj = Path(parsed_start_url.path)
    start_path = str(start_path_obj.parent.as_posix()) if start_path_obj.suffix else str(start_path_obj.as_posix())
    if not start_path.endswith('/'):
        start_path += '/'

    queue = asyncio.Queue()
    visited = set()
    saved_files = []

    # --- Sitemap —Ä–µ–∂–∏–º ---
    if CONFIG.get("CRAWL_MODE") == "Sitemap.xml":
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º URL sitemap.xml
        sitemap_url = start_url.rstrip('/') + "/sitemap.xml" if not start_url.endswith(".xml") else start_url
        console.print(f"[yellow]–ó–∞–≥—Ä—É–∂–∞–µ–º sitemap: {sitemap_url}[/yellow]")
        links = await fetch_sitemap_links(sitemap_url)
        if not links:
            console.print("[red]–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏ –∏–∑ sitemap.xml. –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ä–µ–∂–∏–º—É.[/red]")
            await queue.put((start_url, 0))
        else:
            for link in links:
                await queue.put((link, 0))
    else:
        await queue.put((start_url, 0))

    async with async_playwright() as p:
        console.print("[green]–ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ...[/green]")
        browser = await p.chromium.launch(headless=True, channel="chrome")
        context = await browser.new_context(
            user_agent=FAKE_USER_AGENT,
            extra_http_headers=EXTRA_HEADERS,
        )
        await context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        page = await context.new_page()

        console.print(f"""
[bold]–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:[/bold]
- [cyan]URL:[/cyan] {start_url}
- [cyan]–•–æ—Å—Ç:[/cyan] {start_hostname}
- [cyan]–ü—É—Ç—å:[/cyan] {start_path}*
- [cyan]–ì–ª—É–±–∏–Ω–∞:[/cyan] {CONFIG['MAX_DEPTH']}
- [cyan]–§–æ—Ä–º–∞—Ç:[/cyan] {CONFIG['EXPORT_FORMAT']}
        """)

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
            console=console
        ) as progress:
            
            crawl_task = progress.add_task("[cyan]–û–±—Ö–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü...", total=1)

            while not queue.empty():
                progress.update(crawl_task, total=len(visited) + queue.qsize())

                current_url, current_depth = await queue.get()
                current_url = urljoin(current_url, urlparse(current_url).path)

                if current_url in visited or current_depth > CONFIG["MAX_DEPTH"]:
                    continue

                visited.add(current_url)
                progress.update(crawl_task, description=f"[cyan]–û–±—Ä–∞–±–æ—Ç–∫–∞ ({len(visited)}/{len(visited) + queue.qsize()})[/cyan] [white]{current_url[:70]}...[/white]")

                try:
                    await page.goto(current_url, wait_until="networkidle", timeout=60000)

                    # --- –û–ë–†–ê–ë–û–¢–ö–ê COOKIE –ë–ê–ù–ù–ï–†–ê ---
                    try:
                        accept_button = page.get_by_role(
                            "button",
                            name=re.compile("Accept All Cookies|Accept", re.IGNORECASE)
                        )
                        await accept_button.click(timeout=3000)
                        progress.console.print("[bold green]  ‚úì –ü—Ä–∏–Ω—è–ª–∏ cookie-—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ.[/bold green]")
                        await page.wait_for_timeout(1000)
                    except PlaywrightTimeoutError:
                        pass # –ë–∞–Ω–Ω–µ—Ä–∞ –Ω–µ—Ç, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                    # --------------------------------

                    page_title = await page.title()

                    # –í —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏—â–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏
                    if CONFIG.get("CRAWL_MODE") != "Sitemap.xml" and current_depth < CONFIG["MAX_DEPTH"]:
                        links = await page.eval_on_selector_all("a", "elements => elements.map(el => el.href)")
                        for link in links:
                            absolute_url = urljoin(current_url, link)
                            if is_valid_url(absolute_url, start_hostname, start_path) and absolute_url not in visited and absolute_url not in [item[0] for item in queue._queue]:
                                await queue.put((absolute_url, current_depth + 1))

                    if CONFIG["EXPORT_FORMAT"] == "PDF":
                        pdf_filename = sanitize_filename(page_title) + ".pdf"
                        pdf_filepath = output_path / pdf_filename
                        await page.pdf(path=pdf_filepath, format="A4", print_background=True)
                        saved_files.append(str(pdf_filepath))
                    else: # Text
                        text_content = await page.evaluate("document.body.innerText")
                        md_filename = sanitize_filename(page_title) + ".md"
                        md_filepath = output_path / md_filename
                        with open(md_filepath, "w", encoding="utf-8") as f:
                            f.write(f"# {page_title}\n\n{text_content}")
                        saved_files.append(str(md_filepath))

                except Exception as e:
                    progress.console.print(f"[red]  [!] –û—à–∏–±–∫–∞ –Ω–∞ {current_url}: {e}[/red]")

                progress.update(crawl_task, advance=1)
                await asyncio.sleep(CONFIG["REQUEST_DELAY"])
        
        await context.close()
        await browser.close()

    if saved_files:
        if CONFIG["MERGE_FILES"]:
            console.print(f"\n[bold green]–û–±—Ö–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(saved_files)} —Å—Ç—Ä–∞–Ω–∏—Ü. –ù–∞—á–∏–Ω–∞–µ–º —Å–ª–∏—è–Ω–∏–µ...[/bold green]")
            
            if CONFIG["EXPORT_FORMAT"] == "PDF":
                merger = PdfWriter()
                for pdf_path in sorted(saved_files):
                    try:
                        merger.append(pdf_path)
                    except Exception:
                        console.print(f"[yellow]  [!] –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª {pdf_path}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.[/yellow]")

                merged_filepath = CONFIG["MERGED_FILENAME"]
                merger.write(merged_filepath)
                merger.close()
                console.print(f"[bold magenta]üéâ –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª: {merged_filepath}[/bold magenta]")
            else: # Text merge
                merged_filepath = CONFIG["MERGED_FILENAME"]
                with open(merged_filepath, "w", encoding="utf-8") as merged_file:
                    for file_path in sorted(saved_files):
                        try:
                            with open(file_path, "r", encoding="utf-8") as f:
                                merged_file.write(f.read())
                                merged_file.write("\n\n---\n\n")
                        except Exception as e:
                            console.print(f"[yellow]  [!] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}[/yellow]")
                console.print(f"[bold magenta]üéâ –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª: {merged_filepath}[/bold magenta]")

            if CONFIG["DELETE_INDIVIDUAL_FILES"]:
                console.print("[dim]–£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã...[/dim]")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –∏–∑–±–µ–≥–∞—è –æ—à–∏–±–æ–∫,
                # –µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–º–µ–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞–ª–∏ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª.
                for file_path in set(saved_files):
                    try:
                        os.remove(file_path)
                    except OSError as e:
                        console.print(f"[red]  [!] –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}[/red]")
        else:
            file_type = "PDF-—Ñ–∞–π–ª–æ–≤" if CONFIG["EXPORT_FORMAT"] == "PDF" else "MD-—Ñ–∞–π–ª–æ–≤"
            console.print(f"\n[bold green]–û–±—Ö–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(saved_files)} {file_type} –≤ –ø–∞–ø–∫–µ '{CONFIG['OUTPUT_DIR']}'.[/bold green]")
    else:
        file_type = "PDF" if CONFIG["EXPORT_FORMAT"] == "PDF" else "—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö"
        console.print(f"\n[bold yellow]–ù–µ –±—ã–ª–æ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ {file_type} —Ñ–∞–π–ª–∞.[/bold yellow]")

    console.print("\n[bold]–†–∞–±–æ—Ç–∞ —Å–∫—Ä–∏–ø—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.[/bold]")

if __name__ == "__main__":
    try:
        config = get_user_config()
        asyncio.run(main(config))
    except KeyboardInterrupt:
        console.print("\n\n[bold red]–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.[/bold red]")